{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the medical note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code to import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# As an example for this test, we use the discharge medical note\n",
    "discharge_gz = 'C:/3163dataset/discharge.csv.gz'\n",
    "discharge = pd.read_csv(discharge_gz, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable to store a single patient's medical notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     \\nName:  ___                     Unit No:   _...\n",
       "1     \\nName:  ___                     Unit No:   _...\n",
       "2     \\nName:  ___                     Unit No:   _...\n",
       "3     \\nName:  ___                     Unit No:   _...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_admission_notes = discharge.loc[discharge['subject_id'] == 10000032]['text']\n",
    "patient_admission_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP stage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Model\n",
    "- This uses Hugging Face Transformers' BART model (standard summarizer model)\n",
    "- Install the packages: transformes AND pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to: https://blog.gopenai.com/simplifying-healthcare-text-summarization-of-medical-notes-with-python-391c3a1e738d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pt has HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, PTSD, PTSD. She reported self-discontinuing lasix and spirnolactone because she feels like \"they don't do anything\" and that she \"doesn't want to put more chemicals in her\" In the past week, she notes that she                 has been having worsening abd distension and discomfort. She denies easy bruising, melena, BRBPR,                 hemetesis, hemoptysis, or hematuria. She also had a skin lesion, which was biopsied and showed  skin cancer per patient report. She is not aware of any liver disease or liver disease in her family. Her last alcohol consumption was one drink two months ago. She had food poisoning a week ago from eating stale                 cake (n/v 20 min after food ingestion) She denies other recent illness or sick contacts.\n"
     ]
    }
   ],
   "source": [
    "# This variable stores only one medical note\n",
    "note = patient_admission_notes[0]\n",
    "\n",
    "# INSTALL transformers AND pytorch beforehand\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# BART model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the medical note using the BART model and summarise the note\n",
    "inputs = tokenizer.encode(\"summarize: \" + note, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=500, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result using BART model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pt has HCV cirrhosis c/b ascites, hiv on ART, h/o IVDU, COPD, PTSD, PTSD. She reported self-discontinuing lasix and spirnolactone because she feels like \"they don't do anything\" and that she \"doesn't want to put more chemicals in her\" In the past week, she notes that she                 has been having worsening abd distension and discomfort. She denies easy bruising, melena, BRBPR,                 hemetesis, hemoptysis, or hematuria. She also had a skin lesion, which was biopsied and showed  skin cancer per patient report. She is not aware of any liver disease or liver disease in her family. Her last alcohol consumption was one drink two months ago. She had food poisoning a week ago from eating stale                 cake (n/v 20 min after food ingestion) She denies other recent illness or sick contacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (+) It was able to summarise the very long medical note to a single paragraph in 41.3 seconds\n",
    "- (-) It is not a scientifically-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You need to install sacremoses to use BioGptTokenizer. See https://pypi.org/project/sacremoses/ for installation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ovyli\\anaconda3\\lib\\site-packages\\transformers\\models\\biogpt\\tokenization_biogpt.py:116\u001b[0m, in \u001b[0;36mBioGptTokenizer.__init__\u001b[1;34m(self, vocab_file, merges_file, unk_token, bos_token, eos_token, sep_token, pad_token, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msacremoses\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sacremoses'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, BioGptModel\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# BioGPT\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/biogpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m BioGptModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/biogpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenization and summarisation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ovyli\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:846\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 846\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    850\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    851\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ovyli\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2048\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2045\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2046\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2049\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2050\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2051\u001b[0m     init_configuration,\n\u001b[0;32m   2052\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2053\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2054\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2055\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2056\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2057\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2058\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2060\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ovyli\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2287\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2285\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2287\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2292\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ovyli\\anaconda3\\lib\\site-packages\\transformers\\models\\biogpt\\tokenization_biogpt.py:118\u001b[0m, in \u001b[0;36mBioGptTokenizer.__init__\u001b[1;34m(self, vocab_file, merges_file, unk_token, bos_token, eos_token, sep_token, pad_token, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msacremoses\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install sacremoses to use BioGptTokenizer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pypi.org/project/sacremoses/ for installation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m     )\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msm \u001b[38;5;241m=\u001b[39m sacremoses\n",
      "\u001b[1;31mImportError\u001b[0m: You need to install sacremoses to use BioGptTokenizer. See https://pypi.org/project/sacremoses/ for installation."
     ]
    }
   ],
   "source": [
    "# This variable stores only one medical note\n",
    "note = patient_admission_notes[0]\n",
    "\n",
    "from transformers import AutoTokenizer, BioGptModel\n",
    "\n",
    "# BioGPT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptModel.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# Tokenization and summarisation\n",
    "inputs = tokenizer.encode(\"summarize: \" + note, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=500, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple patient notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
